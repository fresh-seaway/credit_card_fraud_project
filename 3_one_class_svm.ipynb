{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Fraudulent Credit Card Transactions Using a One-Class Support Vector Machine (SVM) Model\n",
    "\n",
    "## Resources \n",
    "\n",
    "- [Geeks for Geeks - Understanding One-Class Support Vector Machines](https://www.geeksforgeeks.org/understanding-one-class-support-vector-machines/)\n",
    "- [Scikit-learn - OneClassSVM Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Variable: Class\n",
      "Input Variables: ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "target_feature = \"Class\"\n",
    "input_features = list(df.columns)\n",
    "input_features.remove(target_feature)\n",
    "\n",
    "print(f\"Target Variable: {target_feature}\")\n",
    "print(f\"Input Variables: {input_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split \n",
    "\n",
    "One-class SVMs are trained only on the \"normal\" class. This requires a little extra work to separate out a training class compromised of only legitimate transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separating features and the target\n",
    "X = df.drop(columns = ['Class'])\n",
    "y = df['Class']\n",
    "\n",
    "# Splitting data into training and testing sets, with only normal transactions (Class = 0) for training\n",
    "X_train = X[y == 0]\n",
    "X_test, y_test = X[y == 1].copy(), y[y == 1].copy()\n",
    "\n",
    "# Include some normal transactions in the test set as well\n",
    "X_train, X_test_normal, y_train, y_test_normal = train_test_split(X_train, \n",
    "                                                                  y[y == 0], \n",
    "                                                                  test_size= 0.2, \n",
    "                                                                  random_state = 42)\n",
    "\n",
    "\n",
    "X_test = np.vstack([X_test, X_test_normal])\n",
    "y_test = np.concatenate([y_test, y_test_normal])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dshebb/miniconda3/envs/dsh-venv/lib/python3.13/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model and Predict Class\n",
    "\n",
    "### One-Class SVM Hyperparameters \n",
    "\n",
    "| Hyperparameter | Default | Other Options | Description |\n",
    "|----------------|---------|---------------|-------------|\n",
    "| kernel         | `rbf`   | `linear`, `poly`, `rbf`, `sigmoid`, `precomputed` | Determines the transformation applied to the input data in a higher-dimensional space. |\n",
    "| degree         | 3       | non-negative number | Only applied for the polynomial kernel to define the function's degree. |  \n",
    "| gamma          | `scale` | `auto`, float | Influences the shape of the decision boundary. A smaller gamma value provides a broader decision boundary, which makes the model less sensitive to individual data points. A larger value creates a more complex decision boundary that is less sensitive to individual boundary points | \n",
    "| nu             | `0.5`   | float (0 - 1) | Provides an upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. It allows users to control the balance between precision and recall in the model. A smaller nu value makes the algorithm more lenient, permitting a higher fraction of margin errors and support vectors, which can be useful in scenarios with a considerable number of anomalies. |\n",
    "\n",
    "\n",
    "### Choosing a kernel function \n",
    "\n",
    "| Kernel    | Description |\n",
    "|-----------|-------------|\n",
    "| Linear Kernel (`linear`)  | Equivalent to performing a linear transformation. Suitable when the relationship between features is approximately linear. The decision boundary in the hyper-dimensional space is a hyperplane. |\n",
    "| Polynomial Kernel (`poly`)    | Introduces non-linearity by considering both the dot product and higher-order interactions between features. Characterized by a user-defined degree parameter (degree). A higher degree allows the model to capture more complex relationships but may increase the risk of overfitting. |\n",
    "| Radial Basis Function (`rbf`)   | Suitable for complex, non-linear relationships. Transforms data into a space where intricate decision boundaries can be draft. Useful when the exact form of relationships is unknown or intricate. |\n",
    "| Sigmoid Kernel (`sigmoid`) | Suitable for scenarios where the data distribution is not well defines or exhibits sigmoidal patterns. Shape and position of the decision boundary are determined by `gamma` and `coef0`/ |\n",
    "\n",
    "\n",
    "\n",
    "### TLDR \n",
    "\n",
    "Let's run with a linear kernel and default values, and see how things go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Instantiate model \n",
    "oc_svm = OneClassSVM(kernel = 'linear', \n",
    "                     nu = 0.5, \n",
    "                     gamma = \"scale\")\n",
    "\n",
    "# Train model\n",
    "oc_svm.fit(X_train_scaled)\n",
    "\n",
    "# Predict outliers in the test set\n",
    "predictions = oc_svm.predict(X_test_scaled)\n",
    "\n",
    "# Convert predictions to binary: 1 for inliers (normal), -1 for outliers (fraud)\n",
    "predictions = (predictions == -1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95     56863\n",
      "           1       0.07      0.91      0.13       492\n",
      "\n",
      "    accuracy                           0.90     57355\n",
      "   macro avg       0.54      0.90      0.54     57355\n",
      "weighted avg       0.99      0.90      0.94     57355\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial result, rbf, nu = .1, gamme = .1\n",
    "```\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.90      0.95     56863\n",
    "           1       0.07      0.91      0.13       492\n",
    "\n",
    "    accuracy                           0.90     57355\n",
    "   macro avg       0.54      0.90      0.54     57355\n",
    "weighted avg       0.99      0.90      0.94     57355\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'nu': [0.1, 0.2, 0.5],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'degree': [2, 3, 4]  # Only relevant for poly kernel\n",
    "}\n",
    "\n",
    "# Cross-Validation and Grid Search\n",
    "grid_search = GridSearchCV(OneClassSVM(), \n",
    "                           param_grid, \n",
    "                           cv = 5, \n",
    "                           scoring='roc_auc', \n",
    "                           verbose = 2, \n",
    "                           n_jobs = -1)\n",
    "\n",
    "grid_search.fit(X_train_scaled)\n",
    "\n",
    "# Best model\n",
    "best_oc_svm = grid_search.best_estimator_\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Predictions\n",
    "predictions = best_oc_svm.predict(X_test_scaled)\n",
    "predictions = (predictions == -1).astype(int)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsh-venv",
   "language": "python",
   "name": "dsh-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
