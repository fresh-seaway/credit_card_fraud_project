{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDBScan Clustering for Credit Card Fraud Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Logger configuration\n",
    "from loguru import logger\n",
    "\n",
    "logger.remove()\n",
    "\n",
    "logger.add(\"logs/hdbscan.log\",\n",
    "           level = \"DEBUG\",\n",
    "           format = \"{time:HH:mm:ss} | {level} | {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (42721, 29)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data  = pd.read_csv(\"../creditcard.csv\")\n",
    "\n",
    "sampled_df, _ = train_test_split(data, \n",
    "                                test_size = 0.85, \n",
    "                                stratify = data['Class'], \n",
    "                                random_state = 42)\n",
    "\n",
    "\n",
    "## Select either sampled or unsampled data\n",
    "# df = data\n",
    "df = sampled_df\n",
    "df_orig = df.copy()\n",
    "\n",
    "## Save labels for later, drop unneeded features  \n",
    "label_df = pd.DataFrame({\"Class\": df[\"Class\"]})\n",
    "df.drop(columns = [\"Class\", \"Time\"], inplace = True)\n",
    "\n",
    "## Scale the amount feature \n",
    "scaler = StandardScaler()\n",
    "df['Amount'] = scaler.fit_transform(df[['Amount']])\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDBSCAN function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "from time import time\n",
    "\n",
    "def hdbscan_clustering(X, params, logging = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        X: a dataframe object with training features \n",
    "        params: a dictionary with model parameters\n",
    "        logging: True or False on whether training information should be logged \n",
    "        \n",
    "    Returns: \n",
    "        The cluster label outputs from hdbscan.fit_predict()\n",
    "            \n",
    "    This function clusters input data using HDBSCAN and returns cluster labels. \n",
    "    The logging parameter can be used to record training time information. \n",
    "    \"\"\"\n",
    "    \n",
    "    ## Set n_jobs to use all available cores\n",
    "    params[\"n_jobs\"] = -1\n",
    "    \n",
    "    ## Cluster it up\n",
    "    start_time = time()                        # Record start time\n",
    "    hbdscan = HDBSCAN(**params)                # Instantiate model with input parameters\n",
    "    cluster_labels = hbdscan.fit_predict(X)    # Assign Cluster Labels \n",
    "    end_time = time()                          # Record End Time  \n",
    "    \n",
    "    ## Record model training time and parameters used \n",
    "    if logging == True:\n",
    "        total_training_time_seconds = end_time - start_time\n",
    "        minutes, seconds = divmod(total_training_time_seconds, 60)\n",
    "        formatted_time = f\"{int(minutes):02}:{int(seconds):02}\"\n",
    "        logger.info(f\"HBDScan Model Trained | Train Time (Minutes): {formatted_time} | Parameters | {params} |\")\n",
    "    \n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clusters(params, label_df, cluster_labels):\n",
    "    \n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - label_df: A dataframe object with a \"Class\" column\n",
    "    - cluster_labels: The label outputs from hdbscan.fit_predict()\n",
    "\n",
    "    For each set of parameters, records:\n",
    "    - the parameters\n",
    "    - the % noise\n",
    "    - Count of fraud in clusters, and \n",
    "    - Number of clusters  \n",
    "    \n",
    "    Logs results in notebooks/hdbscan.log \n",
    "    \"\"\"\n",
    "    \n",
    "    ## Adds cluster labels to common df with class labels \n",
    "    eval_df = label_df.copy()\n",
    "    eval_df[\"Cluster\"] = cluster_labels\n",
    "    \n",
    "    ## Cluster / Class Distribution Table \n",
    "    distribution = eval_df.groupby('Cluster')['Class'].value_counts().unstack(fill_value=0)\n",
    "    \n",
    "    ## Assemble counts  \n",
    "    cluster_count = eval_df[\"Cluster\"].nunique()  \n",
    "    total_noise_count = (eval_df[\"Cluster\"] == -1).sum()\n",
    "    total_fraud_count = (eval_df[\"Class\"] == 1).sum()\n",
    "    noise_fraud_count = distribution.loc[-1, 1]\n",
    "    noise_pct = ((total_noise_count / eval_df.shape[0]) * 100).round(2)\n",
    "    \n",
    "    ## Results\n",
    "    logger.info(f\"Parameters: {params} | Clusters: {cluster_count} | Noise: {noise_pct}% | Fraud in cluster: {total_fraud_count - noise_fraud_count} | Fraud in noise: {noise_fraud_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def cluster_grid_search(X, param_grid, label_df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Accepts X, a parameter grid, and a dataframe with class labels and conducts \n",
    "    a HDBScan model grid search. \n",
    "    \n",
    "    Uses the defined hdbscan_clustering and analyze_clusters function to train, assess, and log data on each parameter set. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract parameters and generate all combinations \n",
    "    keys, values = zip(*param_grid.items())\n",
    "    iter_param = [dict(zip(keys, combo)) for combo in itertools.product(*values)]\n",
    "    \n",
    "    \n",
    "    # Execute Grid Search \n",
    "    combinations = len(iter_param)\n",
    "    logger.info(f\"Grid Search Initiated | Combinations to attempt: {combinations}\")\n",
    "    counter = 1\n",
    "    \n",
    "    for p in iter_param:\n",
    "        logger.info(f\"Search {counter} / {combinations}\")\n",
    "        cluster_labels = hdbscan_clustering(X, p, logging = False)\n",
    "        analyze_clusters(p, label_df, cluster_labels)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "cluster_labels = hdbscan_clustering(df, params, logging = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_clusters(params, label_df, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Grid Search        \n",
    "param_grid = {\n",
    "        'min_cluster_size': [2],\n",
    "        'min_samples': [1],\n",
    "        'cluster_selection_epsilon': [11, 12, 13, 14, 15],\n",
    "    }\n",
    "\n",
    "cluster_grid_search(df, param_grid, label_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsh-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
